{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### prepare.py\n",
        "\n",
        "import torch.utils.data as utils\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def PrepareDataset(historical_data, BATCH_SIZE=48, seq_len=12, pred_len=12, train_propotion=0.7, valid_propotion=0.1):\n",
        "    time_len = historical_data.shape[0]\n",
        "\n",
        "    # MinMax Normalization Method.\n",
        "    for col in historical_data.columns:\n",
        "      col_max = historical_data[col].max()\n",
        "      col_min = historical_data[col].min()\n",
        "      historical_data[col] = (historical_data[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "\n",
        "    price_sequences, price_labels = [], []\n",
        "    for i in range(time_len - seq_len - pred_len):\n",
        "        price_sequences.append(historical_data.iloc[i:i + seq_len].values)\n",
        "        price_labels.append(historical_data.iloc[i + seq_len:i + seq_len + pred_len].values)\n",
        "    price_sequences, price_labels = np.asarray(price_sequences), np.asarray(price_labels)\n",
        "\n",
        "    # Reshape labels to have the same second dimension as the sequences\n",
        "    price_labels = price_labels.reshape(price_labels.shape[0], seq_len, -1)\n",
        "\n",
        "    # shuffle & split the dataset to training and testing sets\n",
        "    sample_size = price_sequences.shape[0]\n",
        "    index = np.arange(sample_size, dtype=int)\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    train_index = int(np.floor(sample_size * train_propotion))\n",
        "    valid_index = int(np.floor(sample_size * (train_propotion + valid_propotion)))\n",
        "\n",
        "    train_data, train_label = price_sequences[:train_index], price_labels[:train_index]\n",
        "    valid_data, valid_label = price_sequences[train_index:valid_index], price_labels[train_index:valid_index]\n",
        "    test_data, test_label = price_sequences[valid_index:], price_labels[valid_index:]\n",
        "\n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "\n",
        "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "    train_dataloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    test_dataloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "Js02UgNJERNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "# Mamba Model Args\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    features: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = \"auto\"\n",
        "    d_conv: int = 4\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        if self.dt_rank == \"auto\":\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "\n",
        "# Pure Mamba network\n",
        "class KFGN_Mamba(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.encode = nn.Linear(args.features, args.d_model)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [ResidualBlock(args) for _ in range(args.n_layer)]\n",
        "        )\n",
        "        self.encoder_norm = RMSNorm(args.d_model)\n",
        "        self.decode = nn.Linear(args.d_model, args.features)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: (batch, seq_len, features)\n",
        "        x = self.encode(input_ids)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = self.encoder_norm(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Residual Block in Mamba Model\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.mixer = MambaBlock(args)\n",
        "        self.norm = RMSNorm(args.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        x_res = x\n",
        "        x_norm = self.norm(x)\n",
        "        x_mixed = self.mixer(x_norm)\n",
        "        # residual connection\n",
        "        output = x_mixed + x_res\n",
        "        return output\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=args.d_inner,\n",
        "            out_channels=args.d_inner,\n",
        "            bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv,\n",
        "            groups=args.d_inner,\n",
        "            padding=args.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        self.x_proj = nn.Linear(\n",
        "            args.d_inner, args.dt_rank + args.d_state * 2, bias=False\n",
        "        )\n",
        "\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), \"n -> d n\", d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        (b, l, d) = x.shape\n",
        "\n",
        "        x_and_res = self.in_proj(x)\n",
        "        (x, res) = x_and_res.split(\n",
        "            split_size=[self.args.d_inner, self.args.d_inner], dim=-1\n",
        "        )\n",
        "\n",
        "        # depthwise conv along sequence\n",
        "        x = rearrange(x, \"b l d_in -> b d_in l\")\n",
        "        x = self.conv1d(x)[:, :, :l]\n",
        "        x = rearrange(x, \"b d_in l -> b l d_in\")\n",
        "\n",
        "        x = F.silu(x)\n",
        "\n",
        "        y = self.ssm(x)\n",
        "\n",
        "        y = y * F.silu(res)\n",
        "\n",
        "        output = self.out_proj(y)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x):\n",
        "        # x: (b, l, d_inner)\n",
        "        (d_in, n) = self.A_log.shape\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        D = self.D.float()\n",
        "\n",
        "        x_dbl = self.x_proj(x)\n",
        "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)\n",
        "\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
        "\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D):\n",
        "        \"\"\"\n",
        "        Standard selective scan without any graph / adjacency mixing.\n",
        "        u:      (b, l, d_in)\n",
        "        delta:  (b, l, d_in)\n",
        "        A:      (d_in, n)\n",
        "        B, C:   (b, l, n)\n",
        "        D:      (d_in,)\n",
        "        \"\"\"\n",
        "        (b, l, d_in) = u.shape\n",
        "        n = A.shape[1]\n",
        "\n",
        "        deltaA = torch.exp(einsum(delta, A, \"b l d_in, d_in n -> b l d_in n\"))\n",
        "        deltaB_u = einsum(\n",
        "            delta, B, u, \"b l d_in, b l n, b l d_in -> b l d_in n\"\n",
        "        )\n",
        "\n",
        "        x = torch.zeros((b, d_in, n), device=u.device, dtype=u.dtype)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            # C[:, i, :] is (b, n)\n",
        "            y = einsum(x, C[:, i, :], \"b d_in n, b n -> b d_in\")\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # (b, l, d_in)\n",
        "\n",
        "        y = y + u * D  # (b, l, d_in)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        output = x * rms * self.weight\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "JvbddnVrbFl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TrainAndTest\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.autograd import Variable\n",
        "\n",
        "y = []\n",
        "\n",
        "def TrainSTG_Mamba(train_dataloader, valid_dataloader, mamba_features, num_epochs=1):\n",
        "    inputs, labels = next(iter(train_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "    input_dim = fea_size\n",
        "    hidden_dim = fea_size\n",
        "    output_dim = fea_size\n",
        "\n",
        "    kfgn_mamba_args = ModelArgs(\n",
        "        d_model=fea_size,\n",
        "        n_layer=4,\n",
        "        features=mamba_features\n",
        "    )\n",
        "\n",
        "    kfgn_mamba = KFGN_Mamba(kfgn_mamba_args)\n",
        "    kfgn_mamba.cuda()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = optim.AdamW(kfgn_mamba.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    interval = 100\n",
        "    losses_train = []\n",
        "    losses_interval_train = []\n",
        "    losses_valid = []\n",
        "    losses_interval_valid = []\n",
        "    losses_epoch = []\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        trained_number = 0\n",
        "\n",
        "        valid_dataloader_iter = iter(valid_dataloader)\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if inputs.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            kfgn_mamba.zero_grad()\n",
        "\n",
        "            labels = torch.squeeze(labels)\n",
        "            pred = kfgn_mamba(inputs)\n",
        "\n",
        "            loss_train = loss_MSE(pred, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            # Update learning rate by CosineAnnealingLR\n",
        "            scheduler.step()\n",
        "\n",
        "            losses_train.append(loss_train.data)\n",
        "\n",
        "            # validation\n",
        "            try:\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "            except StopIteration:\n",
        "                valid_dataloader_iter = iter(valid_dataloader)\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
        "            else:\n",
        "                inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
        "\n",
        "            labels_val = torch.squeeze(labels_val)\n",
        "\n",
        "            pred = kfgn_mamba(inputs_val)\n",
        "            loss_valid = loss_MSE(pred, labels_val)\n",
        "            losses_valid.append(loss_valid.data)\n",
        "\n",
        "            trained_number += 1\n",
        "\n",
        "            if trained_number % interval == 0:\n",
        "                cur_time = time.time()\n",
        "                loss_interval_train = np.around(sum(losses_train[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_train.append(loss_interval_train)\n",
        "                loss_interval_valid = np.around(sum(losses_valid[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_valid.append(loss_interval_valid)\n",
        "                print('Iteration #: {}, train_loss: {}, valid_loss: {}, time: {}'.format(\n",
        "                    trained_number * batch_size,\n",
        "                    loss_interval_train,\n",
        "                    loss_interval_valid,\n",
        "                    np.around([cur_time - pre_time], decimals=8)))\n",
        "                pre_time = cur_time\n",
        "\n",
        "        loss_epoch = loss_valid.cpu().data.numpy()\n",
        "        losses_epoch.append(loss_epoch)\n",
        "\n",
        "    return kfgn_mamba, [losses_train, losses_interval_train, losses_valid, losses_interval_valid]\n",
        "\n",
        "\n",
        "\n",
        "def TestSTG_Mamba(kfgn_mamba, test_dataloader):\n",
        "    inputs, labels = next(iter(test_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    tested_batch = 0\n",
        "\n",
        "    losses_mse = []\n",
        "    losses_l1 = []\n",
        "    MAEs = []\n",
        "    MAPEs = []\n",
        "    MSEs = []\n",
        "    RMSEs = []\n",
        "    VARs = []\n",
        "\n",
        "    for data in test_dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        if inputs.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        pred = kfgn_mamba(inputs)\n",
        "        labels = torch.squeeze(labels)\n",
        "\n",
        "        loss_mse = F.mse_loss(pred, labels)\n",
        "        loss_l1 = F.l1_loss(pred, labels)\n",
        "        MAE = torch.mean(torch.abs(pred - torch.squeeze(labels)))\n",
        "        MAPE = torch.mean(torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels)))\n",
        "        # Calculate MAPE only for non-zero labels\n",
        "        non_zero_labels = torch.abs(labels) > 0\n",
        "        if torch.any(non_zero_labels):\n",
        "            MAPE_values = torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels))\n",
        "            MAPE = torch.mean(MAPE_values[non_zero_labels])\n",
        "            MAPEs.append(MAPE.item())\n",
        "\n",
        "        MSE = torch.mean((torch.squeeze(labels) - pred)**2)\n",
        "        RMSE = math.sqrt(torch.mean((torch.squeeze(labels) - pred)**2))\n",
        "        VAR = 1-(torch.var(torch.squeeze(labels)-pred))/torch.var(torch.squeeze(labels))\n",
        "\n",
        "        losses_mse.append(loss_mse.item())\n",
        "        losses_l1.append(loss_l1.item())\n",
        "        MAEs.append(MAE.item())\n",
        "        MAPEs.append(MAPE.item())\n",
        "        MSEs.append(MSE.item())\n",
        "        RMSEs.append(RMSE)\n",
        "        VARs.append(VAR.item())\n",
        "\n",
        "        y.append(pred.cpu().data.numpy())\n",
        "\n",
        "        tested_batch += 1\n",
        "\n",
        "        if tested_batch % 100 == 0:\n",
        "            cur_time = time.time()\n",
        "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format(\n",
        "                tested_batch * batch_size,\n",
        "                np.around([loss_l1.item()], decimals=8),\n",
        "                np.around([loss_mse.item()], decimals=8),\n",
        "                np.around([cur_time - pre_time], decimals=8)))\n",
        "            pre_time = cur_time\n",
        "\n",
        "    losses_l1 = np.array(losses_l1)\n",
        "    losses_mse = np.array(losses_mse)\n",
        "    MAEs = np.array(MAEs)\n",
        "    MAPEs = np.array(MAPEs)\n",
        "    MSEs = np.array(MSEs)\n",
        "    RMSEs = np.array(RMSEs)\n",
        "    VARs = np.array(VARs)\n",
        "\n",
        "    mean_l1 = np.mean(losses_l1)\n",
        "    std_l1 = np.std(losses_l1)\n",
        "    mean_mse = np.mean(losses_mse)\n",
        "    MAE_ = np.mean(MAEs)\n",
        "    std_MAE_ = np.std(MAEs)\n",
        "    MAPE_ = np.mean(MAPEs) * 100\n",
        "    MSE_ = np.mean(MSEs)\n",
        "    RMSE_ = np.mean(RMSEs)\n",
        "    VAR_ = np.mean(VARs)\n",
        "    results = [MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_]\n",
        "\n",
        "    print('Tested: MAE: {}, std_MAE: {}, MAPE: {}, MSE: {}, RMSE: {}, VAR: {}'.format(MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_))\n",
        "    return results"
      ],
      "metadata": {
        "id": "Mdz3AN8sEmu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py\n",
        "\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "\n",
        "# Define the ticker symbol\n",
        "ticker_symbol = \"AAPL\"\n",
        "\n",
        "# Create a Ticker object\n",
        "ticker = yf.Ticker(ticker_symbol)\n",
        "\n",
        "# Fetch historical market data\n",
        "historical_data = ticker.history(period=\"10y\")  # data for the last year\n",
        "historical_data = historical_data.drop(columns=['Dividends', 'Volume', 'Stock Splits'])\n",
        "\n",
        "print(\"\\nPreparing train/test data...\")\n",
        "train_dataloader, valid_dataloader, test_dataloader = PrepareDataset(historical_data, BATCH_SIZE=2)\n",
        "\n",
        "print(\"\\nTraining STGmamba model...\")\n",
        "STGmamba, STGmamba_loss = TrainSTG_Mamba(train_dataloader, valid_dataloader, num_epochs=25, mamba_features=historical_data.shape[1])\n",
        "print(\"\\nTesting STGmamba model...\")\n",
        "results = TestSTG_Mamba(STGmamba, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmBE1nieF73L",
        "outputId": "d303a0fc-9700-4802-eae8-e45a6dee9daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing train/test data...\n",
            "\n",
            "Training STGmamba model...\n",
            "Epoch 0/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.26905471086502075, valid_loss: 0.8300991058349609, time: [4.76417089]\n",
            "Iteration #: 400, train_loss: 0.24644792079925537, valid_loss: 0.7499955892562866, time: [4.39177775]\n",
            "Iteration #: 600, train_loss: 0.21935178339481354, valid_loss: 0.6625334024429321, time: [4.26953292]\n",
            "Iteration #: 800, train_loss: 0.1852482408285141, valid_loss: 0.5879996418952942, time: [4.85992408]\n",
            "Iteration #: 1000, train_loss: 0.1701454371213913, valid_loss: 0.49794644117355347, time: [4.42805529]\n",
            "Iteration #: 1200, train_loss: 0.12527894973754883, valid_loss: 0.4309365153312683, time: [4.30355382]\n",
            "Iteration #: 1400, train_loss: 0.10572808235883713, valid_loss: 0.3766802251338959, time: [4.95886278]\n",
            "Iteration #: 1600, train_loss: 0.08372972160577774, valid_loss: 0.32421794533729553, time: [4.41648817]\n",
            "Epoch 1/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.07430200278759003, valid_loss: 0.2521275579929352, time: [8.45909476]\n",
            "Iteration #: 400, train_loss: 0.05687129870057106, valid_loss: 0.22233617305755615, time: [4.29282737]\n",
            "Iteration #: 600, train_loss: 0.04606986045837402, valid_loss: 0.19847212731838226, time: [4.92433786]\n",
            "Iteration #: 800, train_loss: 0.048514820635318756, valid_loss: 0.17953656613826752, time: [4.28874016]\n",
            "Iteration #: 1000, train_loss: 0.036520589143037796, valid_loss: 0.16935141384601593, time: [4.28644133]\n",
            "Iteration #: 1200, train_loss: 0.040650561451911926, valid_loss: 0.16082380712032318, time: [4.89601636]\n",
            "Iteration #: 1400, train_loss: 0.037325698882341385, valid_loss: 0.14915452897548676, time: [4.29519439]\n",
            "Iteration #: 1600, train_loss: 0.03162287920713425, valid_loss: 0.14175692200660706, time: [4.75228572]\n",
            "Epoch 2/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.027424760162830353, valid_loss: 0.12883315980434418, time: [7.53277826]\n",
            "Iteration #: 400, train_loss: 0.026924340054392815, valid_loss: 0.12309957295656204, time: [4.86452293]\n",
            "Iteration #: 600, train_loss: 0.02576581947505474, valid_loss: 0.1172121912240982, time: [4.3133316]\n",
            "Iteration #: 800, train_loss: 0.02526113949716091, valid_loss: 0.10961127281188965, time: [4.27607203]\n",
            "Iteration #: 1000, train_loss: 0.025614699348807335, valid_loss: 0.10443665832281113, time: [4.91346884]\n",
            "Iteration #: 1200, train_loss: 0.01934828981757164, valid_loss: 0.09785515815019608, time: [4.31172204]\n",
            "Iteration #: 1400, train_loss: 0.021203739568591118, valid_loss: 0.09387125074863434, time: [4.4519763]\n",
            "Iteration #: 1600, train_loss: 0.01984163001179695, valid_loss: 0.08443763852119446, time: [4.79104733]\n",
            "Epoch 3/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.01688878983259201, valid_loss: 0.07766611129045486, time: [7.57656479]\n",
            "Iteration #: 400, train_loss: 0.015584680251777172, valid_loss: 0.07634176313877106, time: [4.69044399]\n",
            "Iteration #: 600, train_loss: 0.015189990401268005, valid_loss: 0.06906744837760925, time: [4.27588248]\n",
            "Iteration #: 800, train_loss: 0.014482430182397366, valid_loss: 0.06446564197540283, time: [5.06830502]\n",
            "Iteration #: 1000, train_loss: 0.012916279956698418, valid_loss: 0.0627598688006401, time: [4.32340574]\n",
            "Iteration #: 1200, train_loss: 0.012970340438187122, valid_loss: 0.05877375975251198, time: [4.30978942]\n",
            "Iteration #: 1400, train_loss: 0.011142410337924957, valid_loss: 0.05552205815911293, time: [4.86876535]\n",
            "Iteration #: 1600, train_loss: 0.011113259941339493, valid_loss: 0.052742719650268555, time: [4.29806924]\n",
            "Epoch 4/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.009957989677786827, valid_loss: 0.04757858067750931, time: [7.90938187]\n",
            "Iteration #: 400, train_loss: 0.008984060026705265, valid_loss: 0.04391913115978241, time: [4.33465505]\n",
            "Iteration #: 600, train_loss: 0.008080080151557922, valid_loss: 0.04332825168967247, time: [4.82995558]\n",
            "Iteration #: 800, train_loss: 0.008366120047867298, valid_loss: 0.038923248648643494, time: [4.37191391]\n",
            "Iteration #: 1000, train_loss: 0.006994510069489479, valid_loss: 0.036985378712415695, time: [4.31055737]\n",
            "Iteration #: 1200, train_loss: 0.007049900013953447, valid_loss: 0.03470965102314949, time: [4.88126969]\n",
            "Iteration #: 1400, train_loss: 0.005738589912652969, valid_loss: 0.032451361417770386, time: [4.31204367]\n",
            "Iteration #: 1600, train_loss: 0.0053834798745810986, valid_loss: 0.031216619536280632, time: [4.35262418]\n",
            "Epoch 5/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.004491180181503296, valid_loss: 0.027111049741506577, time: [8.01447511]\n",
            "Iteration #: 400, train_loss: 0.004226400051265955, valid_loss: 0.026386799290776253, time: [4.46208358]\n",
            "Iteration #: 600, train_loss: 0.004838219843804836, valid_loss: 0.022889340296387672, time: [4.73913455]\n",
            "Iteration #: 800, train_loss: 0.0033633399289101362, valid_loss: 0.02137162908911705, time: [4.31206322]\n",
            "Iteration #: 1000, train_loss: 0.00379385007545352, valid_loss: 0.02085573971271515, time: [4.89541364]\n",
            "Iteration #: 1200, train_loss: 0.0031583199743181467, valid_loss: 0.01870555989444256, time: [4.42188239]\n",
            "Iteration #: 1400, train_loss: 0.0023574200458824635, valid_loss: 0.016679370775818825, time: [4.30516624]\n",
            "Iteration #: 1600, train_loss: 0.0020941700786352158, valid_loss: 0.016718490049242973, time: [4.93047976]\n",
            "Epoch 6/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0021111699752509594, valid_loss: 0.015041310340166092, time: [7.38076401]\n",
            "Iteration #: 400, train_loss: 0.0019452000269666314, valid_loss: 0.013760119676589966, time: [4.88111138]\n",
            "Iteration #: 600, train_loss: 0.0016153899487107992, valid_loss: 0.012559990398585796, time: [4.30246615]\n",
            "Iteration #: 800, train_loss: 0.0015431400388479233, valid_loss: 0.011874670162796974, time: [4.74844909]\n",
            "Iteration #: 1000, train_loss: 0.001340680057182908, valid_loss: 0.011843600310385227, time: [4.48699737]\n",
            "Iteration #: 1200, train_loss: 0.0014797700569033623, valid_loss: 0.011159139685332775, time: [4.32218146]\n",
            "Iteration #: 1400, train_loss: 0.001500000013038516, valid_loss: 0.010013080202043056, time: [4.88826036]\n",
            "Iteration #: 1600, train_loss: 0.0011998099507763982, valid_loss: 0.008980480022728443, time: [4.34885764]\n",
            "Epoch 7/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0010658899554982781, valid_loss: 0.008462239988148212, time: [7.93085456]\n",
            "Iteration #: 400, train_loss: 0.0010393799748271704, valid_loss: 0.008055070415139198, time: [4.34468436]\n",
            "Iteration #: 600, train_loss: 0.0009042600286193192, valid_loss: 0.00758440000936389, time: [4.54112363]\n",
            "Iteration #: 800, train_loss: 0.0009736099746078253, valid_loss: 0.007691230159252882, time: [4.69736266]\n",
            "Iteration #: 1000, train_loss: 0.0007845599902793765, valid_loss: 0.007251880131661892, time: [4.32163239]\n",
            "Iteration #: 1200, train_loss: 0.0007703899755142629, valid_loss: 0.007040500175207853, time: [4.87035751]\n",
            "Iteration #: 1400, train_loss: 0.000888169975951314, valid_loss: 0.006430639885365963, time: [4.31406188]\n",
            "Iteration #: 1600, train_loss: 0.0006564300274476409, valid_loss: 0.006346779875457287, time: [4.31103683]\n",
            "Epoch 8/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0007622300181537867, valid_loss: 0.0060784597881138325, time: [7.97160602]\n",
            "Iteration #: 400, train_loss: 0.0007579299854114652, valid_loss: 0.006003519985824823, time: [4.32380676]\n",
            "Iteration #: 600, train_loss: 0.0008164099999703467, valid_loss: 0.006223179865628481, time: [4.91303635]\n",
            "Iteration #: 800, train_loss: 0.0005809100111946464, valid_loss: 0.005237040109932423, time: [4.3409462]\n",
            "Iteration #: 1000, train_loss: 0.0007354699773713946, valid_loss: 0.005426169838756323, time: [4.81583643]\n",
            "Iteration #: 1200, train_loss: 0.0006298199878074229, valid_loss: 0.005279129836708307, time: [4.44388556]\n",
            "Iteration #: 1400, train_loss: 0.0005244999774731696, valid_loss: 0.005139209795743227, time: [4.43366122]\n",
            "Iteration #: 1600, train_loss: 0.0005656700232066214, valid_loss: 0.00498558022081852, time: [4.97699356]\n",
            "Epoch 9/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.000661369995214045, valid_loss: 0.004785529803484678, time: [7.42043853]\n",
            "Iteration #: 400, train_loss: 0.0005378599744290113, valid_loss: 0.004861509893089533, time: [4.92354631]\n",
            "Iteration #: 600, train_loss: 0.00045947000035084784, valid_loss: 0.004495520144701004, time: [4.34744835]\n",
            "Iteration #: 800, train_loss: 0.000574979989323765, valid_loss: 0.004772860091179609, time: [4.58279014]\n",
            "Iteration #: 1000, train_loss: 0.0006805399898439646, valid_loss: 0.004587600007653236, time: [4.65001535]\n",
            "Iteration #: 1200, train_loss: 0.0006243499810807407, valid_loss: 0.004664990119636059, time: [4.37366414]\n",
            "Iteration #: 1400, train_loss: 0.0005987500189803541, valid_loss: 0.0047877999022603035, time: [4.90505481]\n",
            "Iteration #: 1600, train_loss: 0.0006921699969097972, valid_loss: 0.004229189828038216, time: [4.35540104]\n",
            "Epoch 10/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.00045371000305749476, valid_loss: 0.004109680186957121, time: [7.951267]\n",
            "Iteration #: 400, train_loss: 0.000582969980314374, valid_loss: 0.0037223699036985636, time: [4.35644197]\n",
            "Iteration #: 600, train_loss: 0.0006037700222805142, valid_loss: 0.0041019800119102, time: [4.29354334]\n",
            "Iteration #: 800, train_loss: 0.0005047500017099082, valid_loss: 0.0038453699089586735, time: [4.95049095]\n",
            "Iteration #: 1000, train_loss: 0.0006303199916146696, valid_loss: 0.0042184097692370415, time: [4.34506965]\n",
            "Iteration #: 1200, train_loss: 0.0005892600165680051, valid_loss: 0.003781690029427409, time: [4.91797662]\n",
            "Iteration #: 1400, train_loss: 0.0005267899832688272, valid_loss: 0.003710529999807477, time: [4.33550143]\n",
            "Iteration #: 1600, train_loss: 0.000555099977646023, valid_loss: 0.00392631022259593, time: [4.32194781]\n",
            "Epoch 11/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0004910199786536396, valid_loss: 0.003940479829907417, time: [7.98933101]\n",
            "Iteration #: 400, train_loss: 0.0005638300208374858, valid_loss: 0.003577210009098053, time: [4.3923254]\n",
            "Iteration #: 600, train_loss: 0.000669059983920306, valid_loss: 0.0033016400411725044, time: [4.88924193]\n",
            "Iteration #: 800, train_loss: 0.0005640099989250302, valid_loss: 0.00341885001398623, time: [4.36604404]\n",
            "Iteration #: 1000, train_loss: 0.00046514999121427536, valid_loss: 0.003439570078626275, time: [4.70051885]\n",
            "Iteration #: 1200, train_loss: 0.0005846400163136423, valid_loss: 0.0036195400170981884, time: [4.55569339]\n",
            "Iteration #: 1400, train_loss: 0.00040759998955763876, valid_loss: 0.0040041799657046795, time: [4.39681602]\n",
            "Iteration #: 1600, train_loss: 0.00048302000504918396, valid_loss: 0.003358029993250966, time: [4.96891785]\n",
            "Epoch 12/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0005643100012093782, valid_loss: 0.00345706008374691, time: [7.402946]\n",
            "Iteration #: 400, train_loss: 0.0004496100009419024, valid_loss: 0.004070790018886328, time: [5.09681678]\n",
            "Iteration #: 600, train_loss: 0.0006312999757938087, valid_loss: 0.0030790099408477545, time: [4.34248519]\n",
            "Iteration #: 800, train_loss: 0.00046797998948022723, valid_loss: 0.00326648005284369, time: [4.56924653]\n",
            "Iteration #: 1000, train_loss: 0.00047771999379619956, valid_loss: 0.003412419930100441, time: [4.69212651]\n",
            "Iteration #: 1200, train_loss: 0.0005128700286149979, valid_loss: 0.0035398199688643217, time: [4.39285517]\n",
            "Iteration #: 1400, train_loss: 0.0005660500028170645, valid_loss: 0.00330053991638124, time: [4.92216635]\n",
            "Iteration #: 1600, train_loss: 0.0004801499890163541, valid_loss: 0.00337864994071424, time: [4.37926221]\n",
            "Epoch 13/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0004916000179946423, valid_loss: 0.003357799956575036, time: [8.01741147]\n",
            "Iteration #: 400, train_loss: 0.0005360600189305842, valid_loss: 0.003400410059839487, time: [4.36696625]\n",
            "Iteration #: 600, train_loss: 0.0004982399987056851, valid_loss: 0.003144429996609688, time: [4.49450946]\n",
            "Iteration #: 800, train_loss: 0.0004927300033159554, valid_loss: 0.003193190088495612, time: [4.88537455]\n",
            "Iteration #: 1000, train_loss: 0.0005257700104266405, valid_loss: 0.002827889984473586, time: [4.32755399]\n",
            "Iteration #: 1200, train_loss: 0.0005064299912191927, valid_loss: 0.0030277299229055643, time: [4.91126442]\n",
            "Iteration #: 1400, train_loss: 0.0005078199901618063, valid_loss: 0.003219539998099208, time: [4.33692598]\n",
            "Iteration #: 1600, train_loss: 0.0005235300050117075, valid_loss: 0.002967689884826541, time: [4.34697247]\n",
            "Epoch 14/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.00041991000762209296, valid_loss: 0.003175100078806281, time: [8.04235864]\n",
            "Iteration #: 400, train_loss: 0.0004029099945910275, valid_loss: 0.003560560056939721, time: [4.37467647]\n",
            "Iteration #: 600, train_loss: 0.000506750016938895, valid_loss: 0.0033946901094168425, time: [4.93364835]\n",
            "Iteration #: 800, train_loss: 0.0004661999992094934, valid_loss: 0.003221570048481226, time: [4.37602735]\n",
            "Iteration #: 1000, train_loss: 0.000548299984075129, valid_loss: 0.003204149892553687, time: [4.86958599]\n",
            "Iteration #: 1200, train_loss: 0.0005162300076335669, valid_loss: 0.003262439975515008, time: [4.40694714]\n",
            "Iteration #: 1400, train_loss: 0.0004612499906215817, valid_loss: 0.002881390042603016, time: [4.44569468]\n",
            "Iteration #: 1600, train_loss: 0.0005711899721063673, valid_loss: 0.002916140016168356, time: [4.9582181]\n",
            "Epoch 15/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0005865099956281483, valid_loss: 0.0026751901023089886, time: [7.46120787]\n",
            "Iteration #: 400, train_loss: 0.0004378400044515729, valid_loss: 0.0028970399871468544, time: [4.98125768]\n",
            "Iteration #: 600, train_loss: 0.000517659995239228, valid_loss: 0.0026126198936253786, time: [4.40113568]\n",
            "Iteration #: 800, train_loss: 0.0004282000008970499, valid_loss: 0.002763160038739443, time: [4.73784328]\n",
            "Iteration #: 1000, train_loss: 0.0005112600047141314, valid_loss: 0.002716229995712638, time: [4.53910589]\n",
            "Iteration #: 1200, train_loss: 0.0004112499882467091, valid_loss: 0.0027920000720769167, time: [4.38932371]\n",
            "Iteration #: 1400, train_loss: 0.0005234300042502582, valid_loss: 0.002664600033313036, time: [4.92598701]\n",
            "Iteration #: 1600, train_loss: 0.00047790000098757446, valid_loss: 0.0030776499770581722, time: [4.36043859]\n",
            "Epoch 16/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0005544599844142795, valid_loss: 0.002669659908860922, time: [8.01727128]\n",
            "Iteration #: 400, train_loss: 0.0004399899917189032, valid_loss: 0.0028768100310117006, time: [4.36476827]\n",
            "Iteration #: 600, train_loss: 0.0005123799783177674, valid_loss: 0.0027208200190216303, time: [4.64058995]\n",
            "Iteration #: 800, train_loss: 0.0004868099931627512, valid_loss: 0.0027266698889434338, time: [4.74687362]\n",
            "Iteration #: 1000, train_loss: 0.0004287300107534975, valid_loss: 0.002937959972769022, time: [4.46169353]\n",
            "Iteration #: 1200, train_loss: 0.0004351999959908426, valid_loss: 0.0028818100690841675, time: [4.97563744]\n",
            "Iteration #: 1400, train_loss: 0.00044413001160137355, valid_loss: 0.0028372500091791153, time: [4.35331106]\n",
            "Iteration #: 1600, train_loss: 0.0005482700071297586, valid_loss: 0.0026322100311517715, time: [4.35134125]\n",
            "Epoch 17/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.00045754000893794, valid_loss: 0.002663280116394162, time: [8.04572845]\n",
            "Iteration #: 400, train_loss: 0.0004933800082653761, valid_loss: 0.002630820032209158, time: [4.58324289]\n",
            "Iteration #: 600, train_loss: 0.0003828599874395877, valid_loss: 0.002499449998140335, time: [4.80241919]\n",
            "Iteration #: 800, train_loss: 0.0005464199930429459, valid_loss: 0.0026239000726491213, time: [4.38470054]\n",
            "Iteration #: 1000, train_loss: 0.0004404099890962243, valid_loss: 0.0025966300163418055, time: [4.90722513]\n",
            "Iteration #: 1200, train_loss: 0.0004623300046660006, valid_loss: 0.0029038600623607635, time: [4.39925838]\n",
            "Iteration #: 1400, train_loss: 0.00046246001147665083, valid_loss: 0.0025637999642640352, time: [4.35742712]\n",
            "Iteration #: 1600, train_loss: 0.0005144199822098017, valid_loss: 0.0025692100170999765, time: [4.99201465]\n",
            "Epoch 18/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.00039343000389635563, valid_loss: 0.0027816800866276026, time: [7.49813056]\n",
            "Iteration #: 400, train_loss: 0.0004502100055105984, valid_loss: 0.0029383099172264338, time: [4.94857717]\n",
            "Iteration #: 600, train_loss: 0.00048104001325555146, valid_loss: 0.0026966799050569534, time: [4.33329368]\n",
            "Iteration #: 800, train_loss: 0.0005048299790360034, valid_loss: 0.0025549400597810745, time: [4.90526867]\n",
            "Iteration #: 1000, train_loss: 0.0005367199773900211, valid_loss: 0.002550029894337058, time: [4.3764472]\n",
            "Iteration #: 1200, train_loss: 0.0004714299866463989, valid_loss: 0.0024619100149720907, time: [4.38270307]\n",
            "Iteration #: 1400, train_loss: 0.0005068699829280376, valid_loss: 0.0025295799132436514, time: [5.03630924]\n",
            "Iteration #: 1600, train_loss: 0.0004841499903704971, valid_loss: 0.0020290599204599857, time: [4.41350508]\n",
            "Epoch 19/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0005024099955335259, valid_loss: 0.0022741099819540977, time: [8.04651833]\n",
            "Iteration #: 400, train_loss: 0.0004472399887163192, valid_loss: 0.0023135500960052013, time: [4.36875391]\n",
            "Iteration #: 600, train_loss: 0.0005487899761646986, valid_loss: 0.002366370055824518, time: [4.98392773]\n",
            "Iteration #: 800, train_loss: 0.00038645000313408673, valid_loss: 0.0027389200404286385, time: [4.376194]\n",
            "Iteration #: 1000, train_loss: 0.00041591000626794994, valid_loss: 0.0023605599999427795, time: [4.36700845]\n",
            "Iteration #: 1200, train_loss: 0.0004529900033958256, valid_loss: 0.0025051399134099483, time: [4.95466161]\n",
            "Iteration #: 1400, train_loss: 0.00048531999345868826, valid_loss: 0.002515380037948489, time: [4.38300323]\n",
            "Iteration #: 1600, train_loss: 0.0004906399990431964, valid_loss: 0.0025844499468803406, time: [4.62742543]\n",
            "Epoch 20/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0005563999875448644, valid_loss: 0.0025064400397241116, time: [7.8124578]\n",
            "Iteration #: 400, train_loss: 0.0004354699922259897, valid_loss: 0.0025365499313920736, time: [4.84477043]\n",
            "Iteration #: 600, train_loss: 0.00044984000851400197, valid_loss: 0.002474620006978512, time: [4.48369765]\n",
            "Iteration #: 800, train_loss: 0.00047823000932112336, valid_loss: 0.0019789100624620914, time: [4.41907668]\n",
            "Iteration #: 1000, train_loss: 0.0004762499884236604, valid_loss: 0.002472379943355918, time: [4.93357444]\n",
            "Iteration #: 1200, train_loss: 0.0004312499950174242, valid_loss: 0.0026430198922753334, time: [4.39994884]\n",
            "Iteration #: 1400, train_loss: 0.0004728199855890125, valid_loss: 0.0024522000458091497, time: [4.58178711]\n",
            "Iteration #: 1600, train_loss: 0.0004522500094026327, valid_loss: 0.002354389987885952, time: [4.83438826]\n",
            "Epoch 21/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.00035600000410340726, valid_loss: 0.0021991198882460594, time: [7.80672383]\n",
            "Iteration #: 400, train_loss: 0.00044043001253157854, valid_loss: 0.0025609899312257767, time: [4.58548546]\n",
            "Iteration #: 600, train_loss: 0.00048327998956665397, valid_loss: 0.002514980034902692, time: [4.56144881]\n",
            "Iteration #: 800, train_loss: 0.0004848100070375949, valid_loss: 0.0023034499026834965, time: [4.96589184]\n",
            "Iteration #: 1000, train_loss: 0.0004126999992877245, valid_loss: 0.002296270104125142, time: [4.37626958]\n",
            "Iteration #: 1200, train_loss: 0.0004853999998886138, valid_loss: 0.0021596900187432766, time: [4.53635621]\n",
            "Iteration #: 1400, train_loss: 0.000435010006185621, valid_loss: 0.0023904400877654552, time: [4.81203866]\n",
            "Iteration #: 1600, train_loss: 0.0005796899786219001, valid_loss: 0.002430829918012023, time: [4.38784623]\n",
            "Epoch 22/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0004182000120636076, valid_loss: 0.002398200100287795, time: [8.01775765]\n",
            "Iteration #: 400, train_loss: 0.0003723700065165758, valid_loss: 0.0023310999386012554, time: [4.38874292]\n",
            "Iteration #: 600, train_loss: 0.0005272100097499788, valid_loss: 0.0024133101105690002, time: [4.98431706]\n",
            "Iteration #: 800, train_loss: 0.0004648299945984036, valid_loss: 0.002322030020877719, time: [4.41987753]\n",
            "Iteration #: 1000, train_loss: 0.0004298799904063344, valid_loss: 0.002597339916974306, time: [4.33867478]\n",
            "Iteration #: 1200, train_loss: 0.00042612000834196806, valid_loss: 0.0024594999849796295, time: [4.95552945]\n",
            "Iteration #: 1400, train_loss: 0.0005075900116935372, valid_loss: 0.002236309926956892, time: [4.36980724]\n",
            "Iteration #: 1600, train_loss: 0.000489929981995374, valid_loss: 0.0021806899458169937, time: [4.95849442]\n",
            "Epoch 23/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.0004713099915534258, valid_loss: 0.002389230066910386, time: [7.51491785]\n",
            "Iteration #: 400, train_loss: 0.0004885100061073899, valid_loss: 0.0021778501104563475, time: [5.02530122]\n",
            "Iteration #: 600, train_loss: 0.00038052001036703587, valid_loss: 0.0024636699818074703, time: [4.4027133]\n",
            "Iteration #: 800, train_loss: 0.0005454099737107754, valid_loss: 0.0023712399415671825, time: [4.3971076]\n",
            "Iteration #: 1000, train_loss: 0.0004452400025911629, valid_loss: 0.0023885099217295647, time: [4.93189359]\n",
            "Iteration #: 1200, train_loss: 0.00047123999684117734, valid_loss: 0.0024440500419586897, time: [4.3825717]\n",
            "Iteration #: 1400, train_loss: 0.0003556600131560117, valid_loss: 0.0026215501129627228, time: [5.03390813]\n",
            "Iteration #: 1600, train_loss: 0.0005035499925725162, valid_loss: 0.0023505499120801687, time: [4.4454298]\n",
            "Epoch 24/24\n",
            "----------\n",
            "Iteration #: 200, train_loss: 0.00042173999827355146, valid_loss: 0.0022684899158775806, time: [8.09287977]\n",
            "Iteration #: 400, train_loss: 0.0004722900048363954, valid_loss: 0.0025742200668901205, time: [4.39131904]\n",
            "Iteration #: 600, train_loss: 0.0004312999953981489, valid_loss: 0.0020542400889098644, time: [4.42220569]\n",
            "Iteration #: 800, train_loss: 0.0003864599857479334, valid_loss: 0.002293260069563985, time: [4.96075511]\n",
            "Iteration #: 1000, train_loss: 0.0005583700258284807, valid_loss: 0.002050090115517378, time: [4.3837254]\n",
            "Iteration #: 1200, train_loss: 0.0005070599727332592, valid_loss: 0.0019853198900818825, time: [5.01487231]\n",
            "Iteration #: 1400, train_loss: 0.00044768000952899456, valid_loss: 0.0021203800570219755, time: [4.36506963]\n",
            "Iteration #: 1600, train_loss: 0.00041566998697817326, valid_loss: 0.0025000700261443853, time: [4.41017628]\n",
            "\n",
            "Testing STGmamba model...\n",
            "Tested #: 200, loss_l1: [0.09245443], loss_mse: [0.00887452], time: [1.22658467]\n",
            "Tested #: 400, loss_l1: [0.22146299], loss_mse: [0.05041137], time: [1.24107885]\n",
            "Tested: MAE: 0.1665397730158515, std_MAE: 0.06614183611909888, MAPE: 21.301070560353345, MSE: 0.037698910574439956, RMSE: 0.18162712130053535, VAR: -0.19339831550437284\n"
          ]
        }
      ]
    }
  ]
}